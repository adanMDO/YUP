{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 138 - Final Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Tyden Rucker\n",
    "- Adan Montes-de-Oca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the utilization of electromyography, an accurate classifier can be constructed to determine the motion of a muscle.\n",
    "\n",
    "- Does EMG accurately capture specific motor function?\n",
    "- What features of EMG are most useful for creating a classifier that predicts these functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Advancements in biomedical engineering have paved the way for innovative uses of electromyography data, such as decoding hand gestures. By training machine learning algorithms on EMG data derived from forearm muscles during these gestures, we seek to create a model that can reliably predict these gestures based on their unique EMG signatures. The potential applications of such a classifier extend from enhancing human-computer interaction to improving assistive technology for individuals with mobility impairments, thereby contributing significantly to the field of bio-signal processing.\n",
    "\n",
    "In recent years, scientists have used different methodologies like the use of MYO Armbands to measure and capture physical arm movements. Such innovative approaches have enabled researchers to engineer robotic arms that authentically emulate human hand movements. It is understood from prior studies that unique patterns of muscle contractions associated with specific gestures, such as thumbs up or down, can be identified through sophisticated algorithms utilizing electromyography data. Therefore, analyzing these EMG signals, we can devise predictive models that can distinguish different arm movements, thereby enhancing the precision of robotic arms.\n",
    "\n",
    "The MYO Armbands exemplify a cost-effective alternative to traditional, expensive EMG experiments, while maintaining a comparable level of accuracy in predicting EMG recordings. These armbands, equipped with eight electrodes, are designed to collect electromyographic data from participants wearing them. Currently, these models exhibit an impressive real-time classification capability of 83%, with ongoing efforts aimed at minimizing errors and refining the predictive model. The scientific community has begun exploring classic statistical methods, with the anticipation that these techniques could further refine these models. This exploratory work not only holds the potential to improve existing models, but also to enrich other ongoing projects, such as the one spearheaded by Umme Zakia's team, which seeks to enhance daily user experiences with robotic hands.\n",
    "\n",
    " **Use inline citation through Markdown footnotes to specify which references support which statements** \n",
    "\n",
    "[^kobylarz]: Kobylarz, Jhonatan, et al. “Thumbs up, Thumbs Down: Non-verbal Human-robot Interaction Through Real-time EMG Classification via Inductive and Supervised Transductive Transfer Learning.” Journal of Ambient Intelligence and Humanized Computing, vol. 11, no. 12, Springer Science+Business Media, Mar. 2020, pp. 6021–31. https://doi.org/10.1007/s12652-020-01852-z.\n",
    "\n",
    "[^zakia]: Zakia, Umme, and Carlo Menon. “Force Myography-Based Human Robot Interactions via Deep Domain Adaptation and Generalization.” Sensors, vol. 22, no. 1, MDPI, Dec. 2021, p. 211. https://doi.org/10.3390/s22010211."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the use of feature selection and various model selection methods, an efficient model can be constructed that maximizes prediction accuracy using an EMG reading. Model selection will capture the best correlation in the dataset, where as feature selection captures the most influential features/dimensions, that accurately predict the label of an EMG reading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The ideal dataset has EMG readings with multiple feature outputs alongside a clear and potentially numerically coded label for each reading, The dataset should include recordings from different genders as well as different races and ages. This will allow the model to best generalize to new data as the data it is trained on will capture potential factors that could contribute to the classification.\n",
    "\n",
    "2. The dataset we found has recordings from 5 people, 3 male and 2 female. The dataset also only consists of thumbs up, thumbs down, and relax classfications. The limitations of this dataset mean that our classifier although possibly accurate may not capture all of the factors that contribute to muscle state classification. The lack of different muscle states and the limitation of muscle state to only thumbs up, thumbs down, and relax, means that our classifier may not potentially be able to generalize to other EMG recordings focused on other aspects of the body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Acknowledge and address any ethics & privacy related issues of your question(s), proposed dataset(s), and/or analyses. Use the information provided in lecture to guide your group discussion and thinking. If you need further guidance, check out [Deon's Ethics Checklist](http://deon.drivendata.org/#data-science-ethics-checklist). In particular:\n",
    "\n",
    "- The names of subjects will be omitted from any literature regarding the dataset used in this project.\n",
    "- Ethics: The names of subjects will be omitted from any literature regarding the dataset used in this project.\n",
    "- We will make sure that we will not influence or manipulate any data in order for our hypothesis to be true. \n",
    "- We intend to utilize data procured from a verifiable and reputable source. We will also meticulously check the credentials and academic standing of the authors to ensure the source's validity and reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Read over the [COGS138 Team Policies](https://github.com/drsimpkins-teaching/cogs138/blob/main/main_project/TeamPolicies.md) individually. Then, include your group’s expectations of one another for successful completion of your COGS108 project below. Discuss and agree on what all of your expectations are. Discuss how your team will communicate throughout the quarter and consider how you will communicate respectfully should conflicts arise. By including each member’s name above and by adding their name to the submission, you are indicating that you have read the COGS108 Team Policies, accept your team’s expectations below, and have every intention to fulfill them. These expectations are for your team’s use and benefit — they won’t be graded for their details.\n",
    "\n",
    "* *Team Expectation 1 To be at meetings on time*\n",
    "* *Team Expectation 2 Everyone will contribute equally to the project.*\n",
    "* *Team Expecation 3 Make it known promptly when conflicts arise.*\n",
    "* *Team Expecation 4 If a group member is struggling reach out to other group members*\n",
    "* *Team Expecation 5 Be honest about abilities to deliver on deadlines*\n",
    "* *Team Expecation 6 Have Fun*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify your team's specific project timeline. An example timeline has been provided. Changes the dates, times, names, and details to fit your group's plan.\n",
    "\n",
    "If you think you will need any special resources or training outside what we have covered in COGS 138 to solve your problem, then your proposal should state these clearly. For example, if you have selected a problem that involves implementing multiple neural networks, please state this so we can make sure you know what you’re doing and so we can point you to resources you will need to implement your project. Note that you are not required to use outside methods.\n",
    "\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 5/23  |  7 PM | Read & Think about COGS 138 expectations; brainstorm topics/questions  | Determine best form of communication; Discuss and decide on final project topic; discuss hypothesis; begin background research | \n",
    "| 5/24  |  All Day |  Do background research on topic | Discuss ideal dataset(s) and ethics; draft project proposal | \n",
    "| 5/25  | 6:30 PM  | Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 5/26  | 12 PM  | Complete Proposal | Meet with Professor/TA   |\n",
    "| 5/30  | 7 PM  | Begin Data Analysis | Discuss/edit Analysis; |\n",
    "| 6/1  | 7 PM  | Complete analysis; Draft results/conclusion/discussion| Discuss/edit full project for project check-in|\n",
    "| 6/15  | Before 6:15 PM  | NA | Turn in Final Project & Group Project Surveys |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
